[ { "title": "Phân tích chi tiết kiến trúc dữ liệu và data pipeline tại ngân hàng – Trường hợp Ngân hàng Alpha", "url": "/posts/phan-tich-kien-truc-du-lieu-ngan-hang-alpha/", "categories": "Data Architecture, Banking", "tags": "case study, databricks, lakehouse", "date": "2025-06-07 00:00:00 +0700", "snippet": "Phân tích chi tiết kiến trúc dữ liệu và data pipeline tại ngân hàng – Trường hợp Ngân hàng Alpha1. Kiến trúc tổng quan ngành ngân hàng tại Việt NamCác ngân hàng hiện đại tại Việt Nam đang áp dụng kiến trúc hybrid – kết hợp hệ thống giao dịch truyền thống (core banking) với hạ tầng cloud-native, gồm: OLTP Systems: Oracle Database, SQL Server dùng cho xử lý giao dịch realtime. Integration Layer: Dùng các middleware như Apache Kafka, MuleSoft hoặc Azure Event Hub để đẩy dữ liệu giao dịch theo thời gian thực về hệ thống trung tâm. Data Lake: Sử dụng Amazon S3 hoặc Azure Data Lake Gen2 để lưu trữ dữ liệu thô (raw zone). Data Warehouse: Redshift, Snowflake, hoặc Databricks Lakehouse (Delta Lake) cho xử lý phân tích. BI Layer: Power BI, Tableau phục vụ báo cáo và dashboard cho từng phòng ban.2. Kiến trúc Lakehouse tại Ngân hàng AlphaNgân hàng Alpha triển khai kiến trúc Lakehouse trên Databricks, cho phép: Ingest dữ liệu từ &amp;gt;50 hệ thống (tài chính, tín dụng, CRM, giao dịch). Dùng Apache Spark Structured Streaming để ingest gần realtime dữ liệu từ Kafka vào Bronze Layer. Áp dụng chuẩn multi-hop architecture: Bronze Layer: lưu dữ liệu thô gốc từ các hệ thống source. Silver Layer: xử lý, làm sạch, chuẩn hóa, enrich thêm metadata. Gold Layer: tổng hợp theo logic nghiệp vụ (ví dụ: daily transaction summary per branch). 3. Data Pipeline phục vụ phòng Tài chínhMục tiêu: Tính toán P&amp;amp;L theo ngày, chi nhánh, sản phẩm. Đối chiếu giữa hệ thống kế toán và hệ thống giao dịch.Kỹ thuật: Source: Truy vấn giao dịch từ core banking (Oracle OLTP), accounting system (SAP hoặc tương đương). Ingestion: CDC bằng Oracle GoldenGate hoặc Debezium → Kafka. Push vào Bronze table dạng Delta bằng Databricks Autoloader. Processing: Silver: chuẩn hóa schema, join với bảng master data như customer, branch, GL code. Gold: tổng hợp theo chiều time, region, product. Serving: Publish bảng Gold sang Power BI hoặc expose qua Databricks SQL Endpoint cho phòng tài chính. 4. Kiến trúc xử lý giao dịch và đồng bộOLTP Layer: Xử lý trực tiếp trên Oracle RAC Cluster. Transaction được ghi log (redo log, archive log) → phục vụ CDC.Ingestion Layer: CDC log được đọc bởi GoldenGate → Kafka Topic. Mỗi Kafka topic tương ứng một entity (transaction, account, customer). Dữ liệu được phân phối đến Databricks hoặc Snowflake.5. Quản lý đồng bộ dữ liệu và chống inconsistencyCác vấn đề thường gặp: Giao dịch rollback nhưng đã ingest. Dữ liệu phân tán → late arriving data. ETL job chạy trước khi upstream job hoàn thành.Cách khắc phục: Áp dụng idempotent writes và merge (upsert) trên Delta Lake. Dùng watermark + window trong Spark Structured Streaming. Cơ chế exactly-once delivery với checkpointing và schema enforcement.6. POS, ATM và ứng dụng ngân hàng sốPOS / ATM: Hệ thống ATM/POS chạy phần mềm C++ hoặc Java trên hệ điều hành nhúng. Giao dịch gửi về core banking qua ISO 8583 protocol (thường dùng TCP socket hoặc web service). Nếu offline: POS ghi giao dịch local, sync lại khi online.Ngân hàng số: Ứng dụng Ngân hàng Alpha dùng backend dựa trên microservice, triển khai trên Kubernetes. Các request được quản lý qua API Gateway (AWS API Gateway hoặc Kong). Toàn bộ event lưu lại bằng Kafka hoặc EventHub để phục vụ audit và phân tích hành vi người dùng.7. Giao dịch online và offline – xử lý kỹ thuật Tình huống Cách xử lý kỹ thuật POS mất mạng Local cache + checksum, sync khi kết nối lại Giao dịch bị treo Retry + Dead-letter queue Trùng lặp giao dịch Transaction ID + Idempotent key ATM ngắt điện Ghi vào flash + batch recover khi khởi động lại 8. Gắn dữ liệu với nghiệp vụ và báo cáoVí dụ: báo cáo doanh thu tín dụng theo vùng miền Gold Layer chứa bảng: loan_summary_gold Partition theo region và transaction_date Power BI truy vấn theo time-series: rolling 30-day avg, YoY growth Các chỉ số KPI tính trực tiếp trên Databricks SQL: DPD, NPL ratioKết luậnNgân hàng Alpha đang áp dụng một mô hình dữ liệu tiên tiến, kết hợp kiến trúc Lakehouse, streaming data ingestion và phân tích theo chiều nghiệp vụ. Điều này giúp họ vừa đáp ứng giao dịch realtime, vừa phục vụ phân tích chiến lược, kiểm soát rủi ro, và trải nghiệm khách hàng tốt hơn." }, { "title": "Azure Data Engineer Roadmap", "url": "/posts/azure-data-engineer-roadmap/", "categories": "Azure, Data Engineering", "tags": "roadmap, learning", "date": "2025-06-04 00:00:00 +0700", "snippet": "Azure Data Engineer Roadmap1. Foundations1.1 Cloud &amp;amp; Azure Fundamentals AZ-900: Microsoft Azure Fundamentals Core concepts: IaaS vs PaaS vs SaaS Azure global infrastructure Core services: Compute, Storage, Networking Resources Azure Fundamentals learning path (Microsoft Learn) “Azure Fundamentals” video series by John Savill (YouTube) 1.2 Data Fundamentals DP-900: Microsoft Azure Data Fundamentals Relational vs non-relational data Batch vs streaming vs real-time Big Data vs analytics Resources Azure Data Fundamentals learning path (Microsoft Learn) Hands-on sandbox labs in Microsoft Learn 2. Core Data Engineering Services Service Role Suggested Learning Path Azure Storage Blob Storage, ADLS Gen2 Quickstarts + hands-on lab on Microsoft Docs Azure SQL Database Managed relational database Provision &amp;amp; query tutorials Azure Data Factory ETL/ELT orchestration Build sample pipelines Azure Databricks Apache Spark analytics Intro notebooks + “Data Engineering with Spark” modules Azure Synapse Analytics Unified analytics (SQL + Spark + Pipelines) Synapse workspace labs + serverless SQL Event Hubs / IoT Hub High-throughput data ingestion End-to-end event-driven pipeline Azure Stream Analytics Real-time stream processing Real-time dashboard over sensor data Hands-on approach for each service: Follow the official “Quickstart” on docs.microsoft.com. Complete a guided lab in Microsoft Learn or GitHub. Build a mini-project (e.g. ingest CSV → transform in Databricks → load to Synapse → visualize in Power BI). 3. Certification &amp;amp; Deep Dives DP-203: Data Engineering on Microsoft Azure Design &amp;amp; implement data storage solutions Develop data processing solutions (batch &amp;amp; streaming) Secure and monitor data solutions Optional Advanced Exams DP-500 (Azure Database Administrator) AZ-304/305 (Azure Solutions Architect – Data focus) 4. Advanced Topics &amp;amp; Best Practices Infrastructure as Code ARM templates, Bicep, Terraform DevOps for Data CI/CD pipelines for Data Factory &amp;amp; Databricks (Azure DevOps or GitHub Actions) Security &amp;amp; Governance Azure Key Vault, RBAC, Azure Policy &amp;amp; Blueprints Performance &amp;amp; Cost Optimization Spark tuning, SQL pool indexing, storage/compute tiering Budgets, cost alerts 5. Build Real-World Projects Data Lake Ingestion Simulate IoT data → ADLS Gen2 → catalog with Purview End-to-End Analytics Sales pipeline → Synapse SQL pool → Power BI dashboard Streaming Analytics Clickstream / telemetry → Event Hubs → Stream Analytics → Cosmos DB Tip: Publish each project to GitHub to showcase your skills.6. Community &amp;amp; Ongoing Learning Blogs &amp;amp; Newsletters Data Engineering on Azure blog Azure Weekly newsletter Forums &amp;amp; Meetups StackOverflow [azure-data-factory] Local Azure / Data Engineering meetups (search Meetup.com for HCMC) Hackathons &amp;amp; Practice Microsoft Data Saturdays Kaggle competitions 7. Suggested 12-Week Study Plan Week Focus Area Weeks 1–2 AZ-900 &amp;amp; DP-900 (Foundations) Weeks 3–4 Azure Storage &amp;amp; SQL Database Weeks 5–6 Azure Data Factory (ETL/ELT patterns) Weeks 7–8 Azure Databricks &amp;amp; Spark Week 9 Azure Synapse Analytics Week 10 DP-203 Exam Prep &amp;amp; Practice Tests Weeks 11–12 Capstone Project &amp;amp; GitHub Portfolio " }, { "title": "Lịch làm việc trong một ngày", "url": "/posts/Post-thu-hai-de-test/", "categories": "getting start, test post", "tags": "should not be seen", "date": "2021-01-01 00:15:00 +0700", "snippet": "1. Tiêu đề mộtĐây là lịch làm việc đầu tiên trong bài viết. Phần kế tiếp sẽ hiển thị trong tiêu đề với đề mục số 2.2. Tiêu đề haiHello tiêu đề hai. Sau đây ta sẽ tiếp tục đến với tiêu đề số 3 nhé. Nhanh thôi. Vì tôi sẽ không làm bạn mất thêm thời gian nên đề tiêu đề số 3 sẽ bắt đầu ngay sau đây. Vâng, không chờ đợi thêm 1 giây nào nữa thì tiêu đề 3 sẽ được bắt đầu như chúng ta dự định. Và sau đây là sự bắt đầu của tiêu đề 3, các bạn sẽ không còn phải chờ đợi thêm một giây nào nữa rồi. À, cảm ơn vì bạn đã cố gắng theo dõi đến phút giây này. Không phụ sự kiên nhẫn đó, tiêu đề 3, phần ngay tiếp sau đây là xuất hiện trong sự vui mừng của đọc giả. Đặc biệt, sau khi tiêu đề 3 bắt đầu tôi sẽ nói về một chủ đề rất hay mà tôi sẽ bàn sau. Ok. Và đây là tiêu đề 3.3. Tiêu đề 3Trong tiêu đề 3 chúng ta sẽ có 3 phần tiêu đề con như sau:a. Tiêu đề con aBạn thấy tiêu đề nhỏ này trong toc (table of content) chứ?b. Tiêu đề con bTiêu đề con cuối cùng, bạn có thấy nó trong toc?Đây là một bức ảnh chốt bài của tiêu đề 34. Phần này thử công thức toán nhéCông thức tổng độ lỗi bình phương:$J(w)=\\dfrac{1}{2}\\sum_{i=1}^N(y^{(i)}-z^{(i)})^2$Mục nhỏ dành cho codeimport numpy as npclass Perceptron(object): &quot;&quot;&quot;Perceptron classifier Parameters ----------- eta: float Learning rate (0 &amp;lt; eta &amp;lt; 1) n_iter: int Passes over the training dataset random_state: int Random number generator seed for reproducible Attributes ----------- w_: 1d-array Weights after fitting errors_: list Number of misclassification in each epoch &quot;&quot;&quot; def __init__(self, eta=0.01, n_iter=50, random_state=1): self.eta = eta self.n_iter = n_iter self.random_state = random_state def fit(self, X, y): &quot;&quot;&quot;Fit training data. Parameters ----------- X: array-like, shape = [n_examples, n_features] Training vectors, where n_examples is the number of examples n_features is the number of features y: array-like, shape = [n_examples] Target values Returns -------- self: object &quot;&quot;&quot; rgen = np.random.RandomState(self.random_state) self.w_ = rgen.normal(loc=0, scale=0.01, size=1+ X.shape[1]) self.errors_ = [] for _ in range(self.n_iter): errors = 0 for xi, target in zip(X, y): update = self.eta * (target - self.predict(xi)) self.w_[1:] += update * xi self.w_[0] += update errors += int(update != 0) self.errors_.append(errorso) return self def net_input(self, X): &quot;&quot;&quot;Calculate net input&quot;&quot;&quot; return np.dot(X, self.w_[1:]) + self.w_[0] def predict(self, X): &quot;&quot;&quot;Return class label after unit step&quot;&quot;&quot; return np.where(self.net_input(X) &amp;gt;= 0, 1, -1)" }, { "title": "First test post", "url": "/posts/my-first-post/", "categories": "Getting, start, jekyll", "tags": "new post, first time", "date": "2020-01-01 00:00:00 +0700", "snippet": "this page will appear in _post folder. This is republic." } ]
